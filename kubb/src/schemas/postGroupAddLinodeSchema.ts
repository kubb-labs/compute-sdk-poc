/**
* Generated by Kubb (https://kubb.dev/).
* Do not edit manually.
*/

import { z } from "zod/v4";

export const postGroupAddLinodePathParamsSchema = z.object({
    "apiVersion": z.enum(["v4", "v4beta"]).describe("__Enum__ Call either the `v4` URL, or `v4beta` for operations still in Beta."),
"groupId": z.coerce.number().int().describe("ID of the placement group to look up. Run the [List placement groups](https://techdocs.akamai.com/linode-api/reference/get-placement-groups) operation and store the `id` for the applicable placement group.")
    })

/**
 * @description The Linode was added successfully.
 */
export const postGroupAddLinode200Schema = z.object({
    "id": z.optional(z.int().describe("The placement group's ID. You need to provide it for all operations that affect it.")),
"is_compliant": z.optional(z.boolean().describe("Whether all of the compute instances in your placement group are compliant. If `true`, all compute instances meet either the grouped-together or spread-apart model, which you determine through your selected `placement_group_type`. If `false`, a compute instance is out of this compliance. For example, assume you've set `anti-affinity:local` as your `placement_group_type` and your group already has three qualifying compute instances on separate hosts, to support the spread-apart model. If a fourth compute instance is assigned that's on the same host as one of the existing three, the placement group is non-compliant. Enforce compliance in your group by setting a `placement_group_policy`.\n\n> ðŸ“˜\n>\n> Fixing compliance is not self-service. You need to wait for our assistance to physically move compute instances to make the group compliant again.")),
"label": z.optional(z.string().min(1).describe("__Filterable__ The unique name set for the placement group. A label has these constraints:\n\n- It needs to begin and end with an alphanumeric character.\n- It can only consist of alphanumeric characters, hyphens (`-`), underscores (`_`) or periods (`.`).")),
"members": z.optional(z.array(z.object({
    "is_compliant": z.optional(z.boolean().describe("The compliance status of each individual compute instance in the placement group.")),
"linode_id": z.optional(z.int().describe("__Read-only__ The unique identifier for a compute instance included in the placement group."))
    })).describe("An array of compute instances included in the placement group.")),
"placement_group_policy": z.optional(z.enum(["strict", "flexible"]).describe("How requests to add future compute instances to your placement group are handled, and whether it remains compliant:\n\n- `strict`. Don't assign a new compute instance if it breaks the grouped-together or spread-apart model set by the `placement_group_type`. Use this to ensure the placement group stays compliant (`is_compliant: true`).\n- `flexible`. Assign a new compute instance, even if it breaks the grouped-together or spread-apart model set by the `placement_group_type`. This makes the group non-compliant (`is_compliant: false`). You need to wait for Akamai to move the offending compute instance to make it compliant again, once the necessary capacity is available in the region. Offers flexibility to add future compute instances if compliance isn't an immediate concern.\n\n<<LB>>\n\n> ðŸ“˜\n>\n> In rare cases, non-compliance can occur with a `strict` placement group if Akamai needs to failover or migrate your compute instances for maintenance. Fixing non-compliance for a `strict` placement group is prioritized over a `flexible` group.")),
"placement_group_type": z.optional(z.enum(["anti_affinity:local"]).describe("__Filterable__, __Read-only__ How compute instances are distributed in your placement group. A `placement_group_type` using anti-affinity (`anti-affinity:local`) places compute instances in separate hosts, but still in the same region. This best supports the spread-apart model for high availability. A `placement_group_type` using affinity places compute instances physically close together, possibly on the same host. This supports the grouped-together model for low-latency.\n\n> ðŸ“˜\n>\n> Currently, only `anti_affinity:local` is available for `placement_group_type`.")),
"region": z.optional(z.string().describe("__Filterable__, __Read-only__ The [region](https://techdocs.akamai.com/linode-api/reference/get-regions) where the placement group was deployed."))
    })

/**
 * @description See [Errors](https://techdocs.akamai.com/linode-api/reference/errors) for the range of possible error response codes.
 */
export const postGroupAddLinodeErrorSchema = z.object({
    "errors": z.optional(z.array(z.object({
    "field": z.optional(z.string().describe("The field in the request that caused this error. This may be a path, separated by periods in the case of nested fields. In some cases this may come back as `null` if the error is not specific to any single element of the request.")),
"reason": z.optional(z.string().describe("What happened to cause this error. In most cases, this can be fixed immediately by changing the data you sent in the request, but in some cases you will be instructed to [Open a support ticket](https://techdocs.akamai.com/linode-api/reference/post-ticket) or perform some other action before you can complete the request successfully."))
    }).describe("An object for describing a single error that occurred during the processing of a request.")))
    })

export const postGroupAddLinodeMutationRequestSchema = z.object({
    "linodes": z.optional(z.array(z.int()).describe("The `linodeId` values for individual compute instances included in the placement group."))
    }).describe("The compute instances included in a placement group.")

export const postGroupAddLinodeMutationResponseSchema = z.lazy(() => postGroupAddLinode200Schema)